Brief Summary of the paper:

1. The paper aims to project the performance of future processors as the end of Moore's law is approaching and
compare the projected gains in performance to historical trends that were observed due to Moore's law up to the point
when the paper was published. Authors also project the ratio of dark silicon on the chip as we are entering the last
generations of Moore's law scaling.

Key ideas of the paper are as follows:
- Performance improvements in the last 5 technology generations (scaling down of CMOS transistors) will not be as
significant as expected, even taking into account the multi-core designs.
- While transistors will be scaling down to 8 nm, due to end of Dennard scaling, the amount of dark silicon on chip will
be increasing significantly, so it will not be possible to power the whole chip.
- Existing CPU and GPU multicore architectures will not be enough to produce speedups comparable to what we
would come to expect from Moore's law.

Key insight is that in order to continue the historical rates of performance improvement, new architecture designs are
needed.

2. Strengths of the paper:
    - Paper considers together the full architectural and microarchitectural design space from transistors all the way
    to chip organization, number of cores and benchmark characteristics in its design, exploring a lot of factors
    influencing performance.
    - Paper considers various topologies for both CPU and GPU.
    - Paper considers both optimistic and conservative estimates for technology improvements.
    - Paper is well written and authors state all assumptions made in all stages of design clearly.


3. Paper's weaknesses:

Authors do not take into account existing massively parallel workloads:
    Authors are considering only one benchmark suite. There are other benchmark suites that could potentially produce
    more optimistic results if they have more workloads that can be massively parallelized. In fact, to quote PARSEC
    website: "The PARSEC suite anticipates this (parallelization) devlopment and tries to avoid a program selection
    which is skewed towards HPC (High Performance Computing)." I want to argue that it is possible that it is exactly
    those (HPC) types of workloads that will dominate the future selection of workloads. I believe that testing GPU
    topologies on non-HPC workloads (which are known to be massively parallel and thus benefit the most from being run
    on a GPU) is not a fair comparison since those workloads are massively parallel and gain the most performance from
    being run on a GPU. I believe that it might have been a better idea to test CPU and GPU separately on different
    workloads. The results might have been different. In other words

Authors are not taking into account workload development trends:
    Authors are not taking into account the evolution of workloads. With the onset of multicores and wider
    adoption of GPGPUs for general compute, it is likely that increasing number of future workloads will be massively
    parallel. Evaluations on such workloads is likely to produce more optimistic outlook.

Authors use existing state of the art to develop Pareto frontiers:
    - When authors derive Pareto frontier, they use optimal design points of existing cores and fit a Paretto frontier
    through them. They then scale the cores according to technology scaling projections. However, this does not take
    into account the qualitative microarchitectural improvements that are likely to happen in the future and those have
    the potential to improve performance beyond what authors project because author's Pareto's frontier will no longer
    be valid.

4. Could I do better:
I do not believe I could do much better. Even with the weaknesses I believe the paper has, it is still a very good
paper.One opportunity I see to do better would be to analyze new types of real workloads that appreared since the paper
came out. Data processing and ML workloads are massively parallel and could potentially offset some of the paper's
conclusions. However, at the time when paper was published, I do not think that one would be able to do a much better
job.


5. I learned about Pareto frontier and how it can be useful in computer architecture research because it can provide a
good way to think about tradeoffs between different elements of the system, a principle that dominates any computer
system. I really liked this way of considering tradeoffs and I can see myself using it in my own work.
    I liked how authors deconstructed the problem into layers, each corresponding to a layer in hardware design, created
projections of performance for each layer and then combined the layers and analyzed the whole system to obtain the
scaled performance.
    I also liked how authors developed new corollaries to Amdahl's Law (Table 3) and how they extended performance
formulas (formulas 1-4 on pages 369-370).


