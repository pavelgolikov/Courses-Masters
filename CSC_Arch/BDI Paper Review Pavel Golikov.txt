BDI paper review by Pavel Golikov (Student Number 994973230)


1. Brief Summary of the paper:

Authors propose a practical and relatively lightweight compression algorithm on-chip data caches. The algorithm works at
cache line granularity. The main idea is that very often the entire cache line be represented as a base number and an
array of differences. To obtain the value of a word at index i, one needs to add the base number with the difference
at index i from the difference array. The key observation is that storing the base and the array of differences takes
less space than storing the line as words themselves. Authors develop this idea further and use two bases instead of one
to cover more applications. Even though the proposed technique is rather lightweight, it still requires some latency to
decompress the data. Because getting data from L1 cache is on the critical path (and increasing latency there will
decrease performance) authors propose to use the technique to compress L2 and L3 caches, and leave L1 uncompressed.
Authors demonstrate that their technique improves performance by 8.1% for single core and 9.5% /11.2% for two/four
cores. For many applications, BDI effectively increases the cache capacity by 1.53X.


Key ideas of the paper are as follows:

- Cache line can be represented as a base number and an array of differences, which constitutes the compression
mechanism. To decompress the cache line compressed with this method, one needs to add the difference to base for each
difference in the differences array, which can be done in parallel, meaning that the decompression can be done
comparably quickly.

- Storing the cache line as a base number and a difference array takes less space than storing the line normally because
the magnitude of the differences is normally significantly smaller than the magnitude of the words themselves.

- Cache compression can significantly decrease the bandwidth of traffic between L2 and L3 caches if both L2 and L3 are
compressed.

Key insight is that expressing the array of words as a base number and an array of differences takes less space than
storing the words themselves because of the differences can often be stored in the smaller integer or float type.


2. Strengths of the paper:

- The proposed technique achieves better ratio between compression ratio and decompression latency compared to other
state of the art works.

- The ideas of applying base-delta compression to CPU caches is a novel one, but at the same time simple and practical.

- The side effect of compressing both L2 and L3 caches is the significant reduction of bandwidth between L2 and
L3 caches.


3. Paper's weaknesses:

- Authors are stating that if the compression technique is applied to L1 cache, then decompression latency is
potentially large enough to negatively influence the performance in the presense of applications that won't benefit
from cache compression. Because of that, authors only consider caches L2 and higher as candidates for compression.
However, it is the L1 cache that suffers the most from conflict misses because of it's small size.

- On a L2 cache miss or a writeback from L1 to L2, it is possible that in order to insert one cache line into L2,
multiple cache lines will need to be evicted. Evicting two lines to accomodate one could increase the latency if the
evicted cache lines will have to be accessed again as those lines will have to be taken from next level cache of memory
with higher latency.

- Historical trends for L2 and L3 caches suggest that the sizes for the caches are increasing, especially for high
performance computers. If these trends will continue, then the impact of the techniques that authors propose will be
diminished in future because, as authors state, the effectiveness of BDI decreases with cache size since the working set
of more applications can now fit inside the cache.


4. Suggestions/Feedback:

- Authors evaluate their techniques on a single core machine as well as on a multi-core machine with 2 and 4 cores. My
question/suggestion to authors would be to evaluate the proposed techniques on a machine with many more cores. Modern
machines have up to 16 cores with SMT multithreading and I would suggest evaluating on those machines. Those machines
usually have larger cache sizes and it would be interesting to see how BDI would perform in the presense of large number
of cores and with large number of applications run concurrently on a multicore system (from the paper I understood that
authors evaluated multicore workloads and pairs of workloads, but did not evaluate their technique on multicore system
with more than 2 workloads running together).

- Can the proposed technique be used to compress/decompress more than 1 cache line? This has potential to increase the
power overhead, but perhaps this could reduce the tag and other metadata overhead for some cache lines and could be
useful in high-performance server-level machines.

- Have authors considered compressing/decompressing objects instead of lines? Applications operate on objects and not
cache lines and as such, it seems to be more natural to try compress entire objects instead of cache lines.

- I liked the thorough comparison with other state of the art techniques as well as explanations of why the results were
as they were. This allowed me to better understand the pros and cons of all techniques and what are the advantages of
BDI over other works.

- I liked how easy the paper was to read and understand.