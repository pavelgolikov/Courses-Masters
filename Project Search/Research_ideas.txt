MAJOR IDEAS:

- Heterogeneous mobile cpus with cores specilizing in popular applications. 
    - Where does time go in mobile workloads execution?
    - Where does energy go when executing mobile workloads?
    - Where does data go?
    - How can we optimize data structures and memory bandwidth in mobile CPUs? Are modern aggressive OOO systems
    useful for current mobile workloads?
    - What is memory traffic like? Is it optimized?
    - What is power consumption like when executing those workloads? Why it is like that? Are there spikes? Why?

- IP-IP communication in smartphone SOC.

- Offloading computations from smartphone to the cloud. When to do it? What is the metric? offloading ML better than
training on device?

- Combine offloading and heterogeneous cores? 


Standard Architectures:

1. Use FPGA as a generic accelerator in a machine and while the data is loading, configure it to have optimal
structure.
    - when is the overhead of compiling fpga not prohibitively expensive compared to just executing on a GPU or 
    another accelerator?
    test

2. Propose FPGA/GPU hybrid. Certain number of cores are there to fit most workloads (floating point/int) and the rest
are FPGA type arrays of gates to fit specific workloads. Maybe make it modular to be able to assemble an accelerator
that fits your needs. Also could make it so that it can turn off unneeded compute resources and only use what is
necessary after a profile run - that way it will save power.

3. Benchmarks for new specialized architectures.

4. Create tools to make recommendations for accelerator design based on workloads?

5. Common accelerator that would be able to accelerate multiple workloads with one architecture?
    - would need to look at modern architectures and characteristics of workloads for each accelerator and 
    identify commonalities and then see if these things can be done on one accelerator.
    - such accelerator would have simplified pipelines and SMs for SIMD processing.
    - isn't fpga such an accelerator?

6. Common control module for system's accelerators. Could it reduce data movement?

7. Interconnect between accelerators to avoid data movement back to memory?
    - identify cases (if they exist) where data is sent to one accelerator, then back to memory, then to another
    accelerator. Propose to remove redundancy and send from one accelerator directly to another.
    - identify data paths where data goes from one accelerator to the next.

8. Why does GPU have to have it's own memory? With enough load latency hiding, can we not use CPU memory for GPU
operations?

9. Look into problem of information movement in modern computers? Look into mobile architectures and try reduce
data movement there to make them more energy efficient.

10. For in-memory compute: look at reordering common workloads so that operators that can be done in memory 
are first in line or last.

11. Memory address load latency predictor. Device that would predict how latency-sensetive the piece of data is and 
how much can we afford to delay or reschedule the load from this address. Then we could put that piece of data into 
appropriate cache and make note of it.

12. A very META study on MLP/ILP/DLP that would "recommend" the type of core that would perform well for a given
workload (will use the core's MLP, ILP, and DLP). For example, if the workload has ILP > 1.5, use 2-issue wide OOO core.
That way the insight will be: given characteristics of workload, this is the type of core you should use. Alternatively,
we could approach this from the core perspective: say, 4-issue wide OOO core should be used when the workload has 
DLP exceeding 3.0. Something along those lines. Does this make sense or do people just use CPU simulator to determine If
their workload will run well or not.

13. Can we use technique similar to video decoders but in computer memory? In order to reduce data movement we 
do not send the whole cache/RAM line, but only a small part that was changed.

------------------------------------------------------------------------------------------------------------------------
Much fewer papers are published on mobile architecture compared to datacenter architectures.
Mobile architecture:

11. Many assume that user experience while browsing is solely affected by network performance. However, advancements 
in LTE network and WiFi speeds have caused the browser to be more compute-bound. As such, the browser relies heavily
on the CPU for its rendering and network processing. The browser typically spawns tens of threads that require a 
large amount of data and code footprints along with frequent inter-process (thread) communication. Moreover, 
most browsers like Chrome rely on an asynchronous execution model that exercises nearly all of the IPs in a mobile SoC,
including the CPU, GPU, video and audio decoders, networking, crypto-engine, and the various communication IP blocks.
    Would it make sense to look into mobile benchmarks and how mobile architecture can be improved to focus on the 
browser?

13. Look at architecture of mobile systems and see if there are optimizations possible across IPs and other components.
    - will need to study workloads (which ones) and see how IPs and other parts of the architecture are interacting to
    see if it is beneficial to connect IPs directly (Rule 8 in Vijay's article).

18. Interfaces for orchestrating Communications in modern architecture (page 13 in 21 Century document).

19. Flip the Amdahl's maxim and think about processing power per memory bandwidth unit. Since memory is bottleneck,
would it make sense to divide out by the bottleneck metric and obtain performance that way?

20. How much metadata is being transferred in modern architectures? Can we reduce the amount of metadata with a
different interface?

21. In memory compute for smartphones for energy efficiency? Are there workloads that would use that?

22. Can we make accelerator use host memory?

23. Can we streamline memory? There is a lot of talk about making processor cores simpler to save on energy, but 
most energy is spent fetching data. Can we make this process more efficient?

24. Can we eliminate indexing lookup or reduce it somehow? That would speed up memory accesses?

25. Include temperature sensor on the smartphone and move the workload accross cores depending on heating?

------------------------------------------------------------------------------------------------------------------------
26. Operating system that can function in several modes. For example the smartphone can function as a smartphone or as
a desktop computer.
